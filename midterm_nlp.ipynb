{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1373314,"sourceType":"datasetVersion","datasetId":798371},{"sourceId":13317360,"sourceType":"datasetVersion","datasetId":8442254}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:02:54.760054Z","iopub.execute_input":"2025-10-10T03:02:54.760837Z","iopub.status.idle":"2025-10-10T03:02:54.776035Z","shell.execute_reply.started":"2025-10-10T03:02:54.760798Z","shell.execute_reply":"2025-10-10T03:02:54.774666Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.bin\n/kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.vec\n/kaggle/input/train-nlp/train.txt\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport random\nimport math\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Word2Vec (optional, only if strategy = word2vec)\ntry:\n    import gensim\n    from gensim.models import Word2Vec\nexcept:\n    gensim = None\n    print(\"‚ö†Ô∏è gensim not available, will use random init\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:02:57.152551Z","iopub.execute_input":"2025-10-10T03:02:57.154033Z","iopub.status.idle":"2025-10-10T03:02:57.161209Z","shell.execute_reply.started":"2025-10-10T03:02:57.153991Z","shell.execute_reply":"2025-10-10T03:02:57.160039Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"SEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üîß Device: {DEVICE}\")\nprint(f\"üîß PyTorch version: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:02:59.459620Z","iopub.execute_input":"2025-10-10T03:02:59.460643Z","iopub.status.idle":"2025-10-10T03:02:59.473365Z","shell.execute_reply.started":"2025-10-10T03:02:59.460604Z","shell.execute_reply":"2025-10-10T03:02:59.471853Z"}},"outputs":[{"name":"stdout","text":"üîß Device: cpu\nüîß PyTorch version: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"## 1.DATA ANALYSIS FUNCTION","metadata":{}},{"cell_type":"code","source":"def analyze_text_file(path, num_lines=1000):\n    \"\"\"\n    Ph√¢n t√≠ch data ƒë·ªÉ quy·∫øt ƒë·ªãnh embedding strategy\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"üîç ANALYZING FILE: {path}\")\n    print(f\"{'='*70}\\n\")\n    \n    # Read file\n    with open(path, 'r', encoding='utf8', errors='ignore') as f:\n        lines = [line.strip() for line in f.readlines()]\n    \n    # Show samples\n    print(\"üìÑ FIRST 5 LINES:\")\n    print(\"-\" * 70)\n    for i, line in enumerate(lines[:5], 1):\n        display = line[:100] + '...' if len(line) > 100 else line\n        print(f\"{i}. {display}\")\n    print()\n    \n    # Combine text\n    text = \" \".join(lines)\n    total_chars = len(text)\n    \n    # Tokenize\n    words = re.findall(r\"\\w+\", text.lower())\n    total_words = len(words)\n    vocab = Counter(words)\n    vocab_size = len(vocab)\n    \n    # Calculate stats\n    avg_word_len = sum(len(w) for w in words) / max(total_words, 1)\n    \n    # Detect language\n    vi_chars = len(re.findall(r\"[ƒÉ√¢ƒë√™√¥∆°∆∞√°√†·∫£√£·∫°√©√®·∫ª·∫Ω·∫π√≥√≤·ªè√µ·ªç√∫√π·ªß≈©·ª•√Ω·ª≥·ª∑·ªπ·ªµ]\", text.lower()))\n    en_chars = len(re.findall(r\"[a-zA-Z]\", text))\n    \n    if vi_chars > 0.05 * en_chars:\n        language = \"Vietnamese\"\n    else:\n        language = \"English\"\n    \n    # Noise detection\n    noise = len(re.findall(r\"[^a-zA-Z0-9\\s.,!?''\\-]\", text))\n    noise_ratio = noise / max(total_chars, 1)\n    \n    # Sentence length\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_lens = [len(s.split()) for s in sentences]\n    avg_sent_len = np.mean(sent_lens) if sent_lens else 0\n    \n    # Top words\n    common_words = vocab.most_common(30)\n    \n    # Print stats\n    print(\"üìä STATISTICS:\")\n    print(\"-\" * 70)\n    print(f\"Total lines:           {len(lines):,}\")\n    print(f\"Total characters:      {total_chars:,}\")\n    print(f\"Total words:           {total_words:,}\")\n    print(f\"Vocab size (unique):   {vocab_size:,}\")\n    print(f\"Avg word length:       {avg_word_len:.2f}\")\n    print(f\"Avg sentence length:   {avg_sent_len:.1f} words\")\n    print(f\"Language:              {language}\")\n    print(f\"Noise ratio:           {noise_ratio:.2%}\")\n    \n    print(f\"\\nüî§ TOP 30 MOST COMMON WORDS:\")\n    print(\"-\" * 70)\n    print([w for w, _ in common_words])\n    \n    # =========================================================================\n    # DECISION LOGIC\n    # =========================================================================\n    print(f\"\\n{'='*70}\")\n    print(\"üí° EMBEDDING STRATEGY RECOMMENDATION:\")\n    print(f\"{'='*70}\\n\")\n    config = {}\n    \n    # Case 1: Vietnamese\n    if language == \"Vietnamese\":\n        print(\"‚úÖ DETECTED: VIETNAMESE TEXT\")\n        print(\"\\nRECOMMENDATION: FastText Vietnamese pretrained\")\n        print(\"\\nReasons:\")\n        print(\"  ‚Ä¢ Vietnamese c·∫ßn embeddings t·ªët cho d·∫•u thanh\")\n        print(\"  ‚Ä¢ FastText Vietnamese 300d r·∫•t hi·ªáu qu·∫£\")\n        print(\"  ‚Ä¢ Download: https://fasttext.cc/docs/en/crawl-vectors.html\")\n        print(\"\\nFallback: Train Word2Vec on data n·∫øu kh√¥ng c√≥ FastText\\n\")\n        \n        config = {\n            'strategy': 'word_level',\n            'embedding_method': 'fasttext_pretrained',\n            'fasttext_path': '/kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.vec',\n            'embed_dim': 300,\n            'min_word_freq': 2,\n            'trainable': True,\n            'fallback': 'word2vec'\n        }\n    \n    # Case 2: Small vocab\n    elif vocab_size < 10000:\n        print(\"‚úÖ CASE: SMALL VOCABULARY (< 10K)\")\n        print(\"\\nRECOMMENDATION: Random Init or GloVe 100d\")\n        print(\"\\nReasons:\")\n        print(\"  ‚Ä¢ Vocab nh·ªè ‚Üí c√≥ th·ªÉ h·ªçc t·ª´ scratch\")\n        print(\"  ‚Ä¢ N·∫øu c√≥ GloVe ‚Üí s·ª≠ d·ª•ng ƒë·ªÉ tƒÉng t·ªëc converge\")\n        print(\"  ‚Ä¢ GloVe 100d nh·∫π, ph√π h·ª£p small vocab\\n\")\n        \n        config = {\n            'strategy': 'word_level',\n            'embedding_method': 'glove_or_random',\n            'glove_path': '/kaggle/input/glove6b100d/glove.6B.100d.txt',\n            'embed_dim': 100,\n            'min_word_freq': 2,\n            'trainable': True,\n            'fallback': 'random_init'\n        }\n    \n    # Case 3: Medium vocab, enough data\n    elif vocab_size < 50000 and total_words > 500000:\n        print(\"‚úÖ CASE: MEDIUM VOCAB (10-50K) + LARGE DATA\")\n        print(\"\\nRECOMMENDATION: FastText English OR train Word2Vec\")\n        print(\"\\nReasons:\")\n        print(\"  ‚Ä¢ Data ƒë·ªß l·ªõn ‚Üí Word2Vec h·ªçc t·ªët t·ª´ domain\")\n        print(\"  ‚Ä¢ FastText 300d n·∫øu mu·ªën t·∫≠n d·ª•ng pretrained\")\n        print(\"  ‚Ä¢ Word2Vec t·ª´ scratch n·∫øu domain-specific\\n\")\n        \n        config = {\n            'strategy': 'word_level',\n            'embedding_method': 'fasttext_or_word2vec',\n            'fasttext_path': '/kaggle/input/fasttext-en/cc.en.300.bin',\n            'embed_dim': 300,\n            'min_word_freq': 3,\n            'trainable': True,\n            'w2v_config': {\n                'vector_size': 300,\n                'window': 5,\n                'min_count': 3,\n                'epochs': 30,\n                'workers': 4\n            }\n        }\n    \n    # Case 4: Medium vocab, less data\n    elif vocab_size < 50000:\n        print(\"‚úÖ CASE: MEDIUM VOCAB (10-50K) + LESS DATA\")\n        print(\"\\nRECOMMENDATION: GloVe 300d or FastText\")\n        print(\"\\nReasons:\")\n        print(\"  ‚Ä¢ Data kh√¥ng ƒë·ªß train Word2Vec t·ªët\")\n        print(\"  ‚Ä¢ Pretrained embeddings gi√∫p generalization\")\n        print(\"  ‚Ä¢ GloVe/FastText coverage t·ªët cho English\\n\")\n        \n        config = {\n            'strategy': 'word_level',\n            'embedding_method': 'glove_or_fasttext',\n            'glove_path': '/kaggle/input/glove840b300d/glove.840B.300d.txt',\n            'fasttext_path': '/kaggle/input/fasttext-en/cc.en.300.bin',\n            'embed_dim': 300,\n            'min_word_freq': 3,\n            'trainable': True,\n            'max_vocab_size': 30000\n        }\n    \n    # Case 5: Large vocab\n    else:\n        print(\"‚úÖ CASE: LARGE VOCABULARY (> 50K)\")\n        print(\"\\nRECOMMENDATION: FastText 300d + Vocab Limit\")\n        print(\"\\nReasons:\")\n        print(\"  ‚Ä¢ Vocab l·ªõn ‚Üí c·∫ßn gi·ªõi h·∫°n\")\n        print(\"  ‚Ä¢ FastText handle OOV t·ªët\")\n        print(\"  ‚Ä¢ Gi·ªØ top frequent words\\n\")\n        \n        config = {\n            'strategy': 'word_level',\n            'embedding_method': 'fasttext',\n            'fasttext_path': '/kaggle/input/fasttext-en/cc.en.300.bin',\n            'embed_dim': 300,\n            'min_word_freq': 5,\n            'trainable': True,\n            'max_vocab_size': 40000\n        }\n    \n    # Print config\n    print(\"üìã CONFIGURATION:\")\n    print(\"-\" * 70)\n    for k, v in config.items():\n        if isinstance(v, dict):\n            print(f\"{k}:\")\n            for k2, v2 in v.items():\n                print(f\"  {k2:20s}: {v2}\")\n        else:\n            print(f\"{k:20s}: {v}\")\n    \n    print(\"\\n\" + \"=\"*70)\n    \n    return text, config, vocab_size, total_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:03.068096Z","iopub.execute_input":"2025-10-10T03:03:03.068467Z","iopub.status.idle":"2025-10-10T03:03:03.091715Z","shell.execute_reply.started":"2025-10-10T03:03:03.068440Z","shell.execute_reply":"2025-10-10T03:03:03.090359Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/train-nlp/train.txt\"\n\n# Uncomment khi c√≥ data th·∫≠t:\ntext, EMBEDDING_CONFIG, vocab_size, total_words = analyze_text_file(DATA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:09.735667Z","iopub.execute_input":"2025-10-10T03:03:09.736070Z","iopub.status.idle":"2025-10-10T03:03:10.581996Z","shell.execute_reply.started":"2025-10-10T03:03:09.736041Z","shell.execute_reply":"2025-10-10T03:03:10.580799Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüîç ANALYZING FILE: /kaggle/input/train-nlp/train.txt\n======================================================================\n\nüìÑ FIRST 5 LINES:\n----------------------------------------------------------------------\n1. \"6 ContributorsCh√∫ng Ta C·ªßa T∆∞∆°ng Lai Lyrics  Li·ªáu mai sau phai v·ªôi mau kh√¥ng b∆∞·ªõc b√™n c·∫°nh nhau (Kh...\n2. \"2 ContributorsChuy·ªán H·ª£p Tan Lyricsƒê√™m nay l·∫∑ng l·∫Ω S∆∞∆°ng m√π v·ªÅ giƒÉng tr√™n m·∫£nh t√¨nh qu√™ C√≥ ai ƒë·ªÉ bu...\n3. \"2 ContributorsT·∫øt N√†y Con S·∫Ω V·ªÅ LyricsT·∫øt n√†y con s·∫Ω v·ªÅ, d·∫´u ·ªü ƒë√¢u con c≈©ng s·∫Ω v·ªÅ V·ªÅ ƒëem h·∫øt chuy·ªán...\n4. \"1 ContributorT·∫øt Nh·ªõ T·ªõi Gi√† LyricsIntro: M·ªói ƒë·ªùi ng∆∞·ªùi ch·ªâ s·ªëng 1 l·∫ßn Vi·ªác g√¨ ng·ªìi ƒë√≥ ph√¢n v√¢n T·∫øt...\n5. \"2 ContributorsV√¨ Ch√≠nh L√† Em Lyrics  D√°ng ai qua, cho anh ng∆° ng·∫©n G√≥t ki√™u sa, cho anh v∆∞∆°ng v·∫•n N...\n\nüìä STATISTICS:\n----------------------------------------------------------------------\nTotal lines:           1,078\nTotal characters:      1,821,664\nTotal words:           414,751\nVocab size (unique):   9,483\nAvg word length:       3.29\nAvg sentence length:   90.7 words\nLanguage:              Vietnamese\nNoise ratio:           17.80%\n\nüî§ TOP 30 MOST COMMON WORDS:\n----------------------------------------------------------------------\n['em', 'anh', 'kh√¥ng', 'l√†', 'c√≥', 'ƒëi', 'y√™u', 'ta', 'm·ªôt', 'ng∆∞·ªùi', 'i', 'nh∆∞', 'm√¨nh', 'cho', 'ƒë√£', 'trong', 'ng√†y', 'c√≤n', 'you', 'ai', 'v√†', 'nh·ªØng', 'v·ªÅ', 'khi', 'nhau', 'l·∫°i', 's·∫Ω', 'ƒë·ªÉ', 't√¨nh', 'm√†']\n\n======================================================================\nüí° EMBEDDING STRATEGY RECOMMENDATION:\n======================================================================\n\n‚úÖ DETECTED: VIETNAMESE TEXT\n\nRECOMMENDATION: FastText Vietnamese pretrained\n\nReasons:\n  ‚Ä¢ Vietnamese c·∫ßn embeddings t·ªët cho d·∫•u thanh\n  ‚Ä¢ FastText Vietnamese 300d r·∫•t hi·ªáu qu·∫£\n  ‚Ä¢ Download: https://fasttext.cc/docs/en/crawl-vectors.html\n\nFallback: Train Word2Vec on data n·∫øu kh√¥ng c√≥ FastText\n\nüìã CONFIGURATION:\n----------------------------------------------------------------------\nstrategy            : word_level\nembedding_method    : fasttext_pretrained\nfasttext_path       : /kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.vec\nembed_dim           : 300\nmin_word_freq       : 2\ntrainable           : True\nfallback            : word2vec\n\n======================================================================\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"## 2.TEXT PREPROCESSING","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    \"\"\"Lo·∫°i b·ªè metadata, chu·∫©n h√≥a kho·∫£ng tr·∫Øng\"\"\"\n    text = re.sub(r'(?i)(contributors|lyrics|intro[:\\-])', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef split_into_sentences(text):\n    \"\"\"T√°ch c√¢u m·ªÅm cho ti·∫øng Vi·ªát\"\"\"\n    sentences = re.split(r'[.!?]+', text)\n    return [s.strip() for s in sentences if len(s.strip()) > 0]\n\ndef tokenize_word_level(sentence):\n    \"\"\"Tokenize gi·ªØ d·∫•u ti·∫øng Vi·ªát v√† d·∫•u c√¢u\"\"\"\n    tokens = re.findall(r\"[A-Za-z√Ä-·ªπ]+|[^\\w\\s]\", sentence)\n    return [t.lower() for t in tokens]\n\ndef load_and_tokenize_data(path, max_len=50):\n    \"\"\"Load v√† tokenize corpus lyric\"\"\"\n    print(\"\\nüìö LOADING & TOKENIZING DATA...\")\n    print(\"-\" * 70)\n    \n    with open(path, 'r', encoding='utf8', errors='ignore') as f:\n        text = f.read()\n\n    text = clean_text(text)\n    sentences = split_into_sentences(text)\n    \n    tokenized_sentences = []\n    for sent in sentences:\n        tokens = tokenize_word_level(sent)\n        if 2 <= len(tokens) <= max_len:\n            tokenized_sentences.append(tokens)\n    \n    print(f\"‚úÖ Sentences: {len(tokenized_sentences):,}\")\n    print(f\"‚úÖ Example: {' '.join(tokenized_sentences[0][:15])}\")\n    \n    return tokenized_sentences\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:14.131390Z","iopub.execute_input":"2025-10-10T03:03:14.131785Z","iopub.status.idle":"2025-10-10T03:03:14.143513Z","shell.execute_reply.started":"2025-10-10T03:03:14.131759Z","shell.execute_reply":"2025-10-10T03:03:14.142111Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"tokenized_sentences = load_and_tokenize_data(DATA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:16.207127Z","iopub.execute_input":"2025-10-10T03:03:16.208826Z","iopub.status.idle":"2025-10-10T03:03:16.837465Z","shell.execute_reply.started":"2025-10-10T03:03:16.208782Z","shell.execute_reply":"2025-10-10T03:03:16.836195Z"}},"outputs":[{"name":"stdout","text":"\nüìö LOADING & TOKENIZING DATA...\n----------------------------------------------------------------------\n‚úÖ Sentences: 2,780\n‚úÖ Example: \" ch√∫ng ta c·ªßa t∆∞∆°ng lai li·ªáu mai sau phai v·ªôi mau kh√¥ng b∆∞·ªõc b√™n\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"##  3.BUILD VOCABULARY","metadata":{}},{"cell_type":"code","source":"def build_vocabulary(sentences, min_freq=2, max_vocab_size=None):\n    \"\"\"Build vocabulary\"\"\"\n    print(\"\\nüìñ BUILDING VOCABULARY...\")\n    print(\"-\" * 70)\n    \n    counter = Counter()\n    for sent in sentences:\n        counter.update(sent)\n    \n    # Filter by frequency\n    vocab_words = [w for w, c in counter.items() if c >= min_freq]\n    \n    # Limit vocab size\n    if max_vocab_size and len(vocab_words) > max_vocab_size:\n        print(f\"‚ö†Ô∏è  Limiting vocab: {len(vocab_words)} ‚Üí {max_vocab_size}\")\n        vocab_words = [w for w, _ in counter.most_common(max_vocab_size)]\n    \n    # Special tokens\n    special = [\"<pad>\", \"<unk>\"]\n    vocab = special + sorted(vocab_words)\n    \n    stoi = {w: i for i, w in enumerate(vocab)}\n    itos = vocab\n    \n    print(f\"‚úÖ Vocab size: {len(vocab):,}\")\n    print(f\"   Min frequency: {min_freq}\")\n    \n    # Coverage\n    total = sum(counter.values())\n    covered = sum(counter[w] for w in vocab_words)\n    print(f\"   Coverage: {covered/total:.2%}\")\n    \n    return stoi, itos, counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:27.500004Z","iopub.execute_input":"2025-10-10T03:03:27.500537Z","iopub.status.idle":"2025-10-10T03:03:27.511150Z","shell.execute_reply.started":"2025-10-10T03:03:27.500503Z","shell.execute_reply":"2025-10-10T03:03:27.510047Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"MIN_FREQ = 2\nMAX_VOCAB_SIZE = None # Kh√¥ng gi·ªõi h·∫°n\n\nstoi, itos, counter = build_vocabulary(\n    sentences=tokenized_sentences, \n    min_freq=MIN_FREQ, \n    max_vocab_size=MAX_VOCAB_SIZE\n)\n\nif stoi and itos:\n    print(\"\\n--- K·∫øt qu·∫£ tr·∫£ v·ªÅ ---\")\n    print(f\"K√≠ch th∆∞·ªõc Vocab (itos): {len(itos)}\")\n    print(f\"V√≠ d·ª• 10 t·ª´ ƒë·∫ßu ti√™n trong vocab: {itos[:10]}\")\n    \n    # Ki·ªÉm tra √°nh x·∫° t·ª´-sang-s·ªë (string-to-index)\n    word_example = itos[10] # L·∫•y m·ªôt t·ª´ ng·∫´u nhi√™n\n    index_example = stoi[word_example]\n    print(f\"V√≠ d·ª• √°nh x·∫°: '{word_example}' -> {index_example}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:29.852129Z","iopub.execute_input":"2025-10-10T03:03:29.852625Z","iopub.status.idle":"2025-10-10T03:03:29.872699Z","shell.execute_reply.started":"2025-10-10T03:03:29.852598Z","shell.execute_reply":"2025-10-10T03:03:29.871395Z"}},"outputs":[{"name":"stdout","text":"\nüìñ BUILDING VOCABULARY...\n----------------------------------------------------------------------\n‚úÖ Vocab size: 2,039\n   Min frequency: 2\n   Coverage: 97.29%\n\n--- K·∫øt qu·∫£ tr·∫£ v·ªÅ ---\nK√≠ch th∆∞·ªõc Vocab (itos): 2039\nV√≠ d·ª• 10 t·ª´ ƒë·∫ßu ti√™n trong vocab: ['<pad>', '<unk>', '\"', '&', \"'\", '(', ')', '*', ',', '-']\nV√≠ d·ª• √°nh x·∫°: '/' -> 10\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## 4. LOAD PRETRAINED EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\ndef load_fasttext_embeddings(fasttext_path, stoi, embed_dim=300):\n    \"\"\"Load FastText pretrained .vec embeddings (text format)\"\"\"\n    print(f\"\\nüé® LOADING FASTTEXT (.vec): {fasttext_path}\")\n    print(\"-\" * 70)\n    \n    if not os.path.exists(fasttext_path):\n        print(f\"‚ùå File not found, using random init\")\n        return None\n    \n    vocab_size = len(stoi)\n    embedding_matrix = np.random.randn(vocab_size, embed_dim).astype(np.float32) * 0.01\n    \n    found = 0\n    with open(fasttext_path, 'r', encoding='utf8', errors='ignore') as f:\n        first_line = f.readline()\n        # M·ªôt s·ªë file .vec c√≥ header: \"vocab_size dim\"\n        if len(first_line.split()) != embed_dim + 1:\n            f.seek(0)\n        else:\n            print(f\"Header detected: {first_line.strip()}\")\n        \n        for line in f:\n            parts = line.rstrip().split(' ')\n            if len(parts) < embed_dim + 1:\n                continue\n            word = parts[0]\n            if word in stoi:\n                vec = np.asarray(parts[1:], dtype=np.float32)\n                embedding_matrix[stoi[word]] = vec\n                found += 1\n    \n    embedding_matrix[0] = 0.0  # padding token\n    print(f\"‚úÖ Found {found:,}/{vocab_size:,} pretrained vectors ({found/vocab_size:.1%} coverage)\")\n    print(f\"‚úÖ Embedding matrix shape: {embedding_matrix.shape}\")\n    \n    return embedding_matrix\n\n\ndef initialize_embeddings(stoi, config):\n    \"\"\"\n    Initialize embedding matrix based on config\n    \"\"\"\n    embed_dim = config.get('embed_dim', 300)\n    embedding_method = config.get('embedding_method', 'random')\n    vocab_size = len(stoi)\n\n    print(f\"\\nüöÄ Initializing embeddings using method: {embedding_method}\")\n\n    if embedding_method == 'fasttext_pretrained':\n        fasttext_path = config.get('fasttext_path')\n        embedding_matrix = load_fasttext_embeddings(fasttext_path, stoi, embed_dim)\n        if embedding_matrix is None:\n            print(\"‚ö†Ô∏è Falling back to random initialization\")\n            embedding_matrix = np.random.randn(vocab_size, embed_dim).astype(np.float32) * 0.01\n    else:\n        print(\"‚ÑπÔ∏è Using random initialization (no pretrained embeddings)\")\n        embedding_matrix = np.random.randn(vocab_size, embed_dim).astype(np.float32) * 0.01\n\n    # ƒê·∫∑t vector cho padding token (n·∫øu c√≥ index 0)\n    embedding_matrix[0] = 0.0\n    print(f\"‚úÖ Final embedding matrix shape: {embedding_matrix.shape}\")\n\n    return embedding_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:38.043875Z","iopub.execute_input":"2025-10-10T03:03:38.044237Z","iopub.status.idle":"2025-10-10T03:03:38.057758Z","shell.execute_reply.started":"2025-10-10T03:03:38.044216Z","shell.execute_reply":"2025-10-10T03:03:38.056686Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"config = {\n    'embedding_method': 'fasttext_pretrained',\n    'fasttext_path': '/kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.vec',\n    'embed_dim': 300,\n    'min_word_freq': 2,\n    'trainable': True\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:39.944461Z","iopub.execute_input":"2025-10-10T03:03:39.945773Z","iopub.status.idle":"2025-10-10T03:03:39.951206Z","shell.execute_reply.started":"2025-10-10T03:03:39.945736Z","shell.execute_reply":"2025-10-10T03:03:39.949750Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"embedding_matrix = initialize_embeddings(stoi, config)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:03:42.516472Z","iopub.execute_input":"2025-10-10T03:03:42.517585Z","iopub.status.idle":"2025-10-10T03:04:29.092677Z","shell.execute_reply.started":"2025-10-10T03:03:42.517548Z","shell.execute_reply":"2025-10-10T03:04:29.091498Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Initializing embeddings using method: fasttext_pretrained\n\nüé® LOADING FASTTEXT (.vec): /kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.vec\n----------------------------------------------------------------------\n‚úÖ Found 2,005/2,039 pretrained vectors (98.3% coverage)\n‚úÖ Embedding matrix shape: (2039, 300)\n‚úÖ Final embedding matrix shape: (2039, 300)\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"print(embedding_matrix.shape)\nprint(embedding_matrix[:2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:04:37.237999Z","iopub.execute_input":"2025-10-10T03:04:37.238362Z","iopub.status.idle":"2025-10-10T03:04:37.250244Z","shell.execute_reply.started":"2025-10-10T03:04:37.238330Z","shell.execute_reply":"2025-10-10T03:04:37.248660Z"}},"outputs":[{"name":"stdout","text":"(2039, 300)\n[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n [-8.28994997e-03 -5.60181029e-03  7.47293560e-03  6.10370282e-03\n  -2.09015940e-04  1.17327378e-03  1.27766486e-02 -5.91571396e-03\n   5.47097391e-03 -2.02192646e-03 -2.17681192e-03  1.09877679e-02\n   8.25416297e-03  8.13509617e-03  1.30547881e-02  2.10038415e-04\n   6.81952946e-03 -3.10266763e-03  3.24166357e-03 -1.30143063e-03\n   9.69959598e-04  5.95157035e-03 -8.18220619e-03  2.09238715e-02\n  -1.00601735e-02 -1.21418852e-02  1.15811080e-02  7.91662652e-03\n   6.24119816e-03  6.28345460e-03 -1.22467725e-04 -8.97254329e-03\n   7.58045586e-04 -6.77161664e-03  9.75119695e-03 -1.47057383e-03\n  -8.25497229e-03 -3.21385823e-03  4.12931433e-03 -5.63724572e-03\n  -8.22220370e-03  2.43687211e-03  2.44966568e-03 -5.06943138e-03\n  -4.71038278e-03  2.32049939e-03 -1.44808432e-02 -1.40746376e-02\n  -7.18444213e-03 -2.13447143e-03  3.10907559e-03  1.47535615e-02\n   8.57659616e-03 -1.59938529e-03 -1.90162071e-04 -1.00252936e-02\n  -1.85131357e-04 -2.88658636e-03  3.22718546e-03 -8.27230886e-03\n   5.19346539e-03  1.53273894e-02 -1.08760141e-03  4.01711743e-03\n   6.90143974e-03 -4.01220471e-03  2.24092486e-03  1.25923994e-04\n   9.76760988e-04 -7.73009751e-03  2.45101750e-04  4.97998297e-03\n   1.45114362e-02  9.59270820e-03  2.15318240e-02 -7.67347543e-03\n   8.72320589e-03  1.83342001e-03  2.18980275e-02 -8.08298308e-03\n  -8.39721877e-03 -5.99392643e-03 -2.12389566e-02 -5.25755016e-03\n  -7.59132672e-03  1.50393776e-03  3.41755990e-03  1.87617075e-02\n   9.50423814e-03 -5.76903624e-03 -8.98414664e-03  4.91919136e-03\n  -1.32023320e-02  1.83145870e-02  1.17944013e-02 -4.69175633e-03\n  -1.71313453e-02  1.35387238e-02 -1.14539848e-03  1.23781627e-02\n  -1.59442760e-02 -5.99374995e-03  5.24369934e-05  4.69805935e-04\n  -4.50065453e-03  6.22849911e-03 -1.06762033e-02 -1.42379478e-03\n   1.20295631e-03  5.14438795e-03  7.11614871e-03 -1.12464214e-02\n  -1.53411413e-02  1.27767678e-02  3.32314009e-03 -7.48486491e-03\n   1.55115193e-02  1.15674629e-03  1.17929718e-02  6.75184769e-04\n   2.06074789e-02  1.75534077e-02 -2.48964131e-03  9.71570984e-03\n   6.45375950e-03  1.36863161e-02 -9.64923389e-03  6.86051464e-03\n   1.05842445e-02 -1.75873935e-02 -1.18325846e-02 -2.03923229e-02\n  -2.69406824e-03  7.17542227e-03  1.50235696e-02  7.40947784e-04\n   1.62861552e-02 -1.38010141e-02 -1.70338247e-02 -5.55476989e-04\n   3.84065439e-03 -3.26947484e-04 -2.06744205e-02 -8.91200383e-04\n  -1.30446944e-02  6.69672526e-03  3.66598251e-03 -9.39879753e-03\n  -5.13866870e-03 -1.05921347e-02 -6.26790978e-04  9.55142267e-03\n  -9.85726062e-03  5.04046492e-03 -5.30257635e-03 -7.92872813e-03\n  -1.07030361e-03 -1.03524234e-02 -5.53649291e-03 -1.19787790e-02\n   1.96472518e-02  3.52635514e-04 -6.99725514e-03  2.13979906e-03\n  -1.12328050e-03 -2.20969599e-03  6.14166679e-03  7.57507654e-03\n  -5.30501129e-03 -5.75818215e-03 -2.75051687e-03 -2.30192114e-02\n  -1.51519105e-02  1.36687420e-02  1.64496768e-02 -2.49036029e-03\n   5.76556986e-03  3.11250146e-03  3.07888072e-02  1.11957490e-02\n  -1.27917586e-03 -9.55540407e-03 -1.60644632e-02  2.03463621e-03\n  -7.56350718e-03 -1.42225372e-02 -6.46572886e-03 -1.08154798e-02\n   1.68714169e-02  8.81639775e-03 -7.97264074e-05  1.47994412e-02\n   7.73683016e-04 -8.61284137e-03  1.52312405e-02  5.38910041e-03\n  -1.03724608e-02 -1.90338667e-03 -8.75618216e-03 -1.38279973e-02\n   9.26177576e-03  1.90941654e-02 -1.39856748e-02  5.62969176e-03\n  -6.50642579e-03 -4.87125386e-03 -5.92393940e-03 -8.63990746e-03\n   4.85216267e-04 -8.30950122e-03  2.70456821e-03 -5.02381066e-04\n  -2.38948036e-03 -9.07563698e-03 -5.76771284e-03  7.55391223e-03\n   5.00917202e-03 -9.77555197e-03  9.93323047e-04  7.51387095e-03\n  -1.66940521e-02  5.43360179e-03 -6.62623765e-03  5.70598664e-03\n  -7.63259176e-03 -1.80488210e-02 -1.62754245e-02  4.80849470e-04\n   2.59722490e-03 -9.04316548e-03  6.38592476e-03 -1.66151989e-02\n  -6.60797930e-04 -1.21101616e-02 -6.51836069e-03  4.73986700e-04\n  -8.60413350e-03 -3.84555547e-03  1.00629283e-02 -5.76891890e-03\n   8.35692044e-03 -1.12970686e-02  5.29804174e-03  1.44156860e-02\n  -2.47164443e-02 -7.96895288e-03  5.77072147e-03 -2.03045388e-03\n   3.71145876e-03 -6.03985181e-03  8.65897862e-04 -1.55677227e-03\n   1.16778202e-02  2.54420843e-03  3.37602664e-03 -4.11876990e-03\n  -4.87606227e-03 -4.32558171e-03  3.94452130e-03 -4.20984486e-03\n   2.89774849e-03  2.07540076e-02  8.71124677e-03 -3.26023507e-03\n   1.20121390e-02 -4.08075331e-03 -2.03812458e-02 -1.00808628e-02\n  -1.87079180e-02 -3.51513457e-03  1.84183780e-04  1.67643726e-02\n   3.26927355e-03 -2.19100527e-03  8.29405617e-03 -2.21113525e-02\n   2.35614553e-03  7.70865194e-03 -1.47858616e-02  1.14375399e-02\n   3.38496407e-03 -4.15287912e-03  6.32781861e-03  2.27069277e-02\n   1.81866251e-03  2.48220586e-03 -4.59360890e-03 -8.49844422e-03\n   8.30335822e-03 -8.56083818e-03  7.15662376e-04 -4.77657421e-03\n   4.78979805e-03  3.33662075e-03  1.03753991e-02 -5.10016363e-03\n  -2.69874930e-03 -9.78763681e-03 -4.44293255e-03  3.77300498e-03]]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.DATASET (Sliding Window)","metadata":{}},{"cell_type":"code","source":"def create_sliding_window_dataset(sentences, stoi, max_len=40):\n    \"\"\"Create sliding window training examples\"\"\"\n    print(f\"\\nü™ü CREATING SLIDING WINDOWS (max_len={max_len})...\")\n    print(\"-\" * 70)\n    \n    PAD_ID = stoi[\"<pad>\"]\n    UNK_ID = stoi[\"<unk>\"]\n    \n    inputs, targets = [], []\n    \n    for sent in sentences:\n        ids = [stoi.get(w, UNK_ID) for w in sent]\n        \n        for start in range(len(ids)):\n            window = ids[start:start + max_len + 1]\n            if len(window) < 2:\n                continue\n            \n            inp = window[:-1]\n            tgt = window[1:]\n            \n            # Left-pad\n            inp_padded = [PAD_ID] * (max_len - len(inp)) + inp\n            tgt_padded = [PAD_ID] * (max_len - len(tgt)) + tgt\n            \n            inputs.append(inp_padded)\n            targets.append(tgt_padded)\n    \n    X = np.array(inputs, dtype=np.int64)\n    Y = np.array(targets, dtype=np.int64)\n    \n    print(f\"‚úÖ Examples: {len(X):,}\")\n    print(f\"   Shape: {X.shape}\")\n    \n    return X, Y\n\n\nclass TextDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = torch.from_numpy(X).long()\n        self.Y = torch.from_numpy(Y).long()\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.Y[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:04:43.875570Z","iopub.execute_input":"2025-10-10T03:04:43.877107Z","iopub.status.idle":"2025-10-10T03:04:43.889668Z","shell.execute_reply.started":"2025-10-10T03:04:43.877066Z","shell.execute_reply":"2025-10-10T03:04:43.887967Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# X√°c ƒë·ªãnh chi·ªÅu d√†i t·ªëi ƒëa c·ªßa c·ª≠a s·ªï tr∆∞·ª£t (ng·ªØ c·∫£nh)\n# Gi√° tr·ªã n√†y n√™n kh·ªõp v·ªõi SEQUENCE_LENGTH b·∫°n ƒë·ªãnh d√πng trong Model (CELL 8)\nSEQUENCE_LENGTH = 40\n# L·∫•y c√°c bi·∫øn t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc:\n# 1. tokenized_sentences (t·ª´ load_and_tokenize_data)\n# 2. stoi (t·ª´ build_vocabulary)\n\nX, Y = create_sliding_window_dataset(\n    sentences=tokenized_sentences, \n    stoi=stoi, \n    max_len=SEQUENCE_LENGTH # S·ª≠ d·ª•ng bi·∫øn SEQUENCE_LENGTH ƒë√£ khai b√°o\n)\n\n# T·∫°o PyTorch Dataset\nfull_dataset = TextDataset(X, Y)\n\nprint(f\"‚úÖ ƒê√£ t·∫°o Dataset PyTorch v·ªõi {len(full_dataset):,} v√≠ d·ª• hu·∫•n luy·ªán.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:04:53.119823Z","iopub.execute_input":"2025-10-10T03:04:53.121032Z","iopub.status.idle":"2025-10-10T03:04:53.538246Z","shell.execute_reply.started":"2025-10-10T03:04:53.120990Z","shell.execute_reply":"2025-10-10T03:04:53.536883Z"}},"outputs":[{"name":"stdout","text":"\nü™ü CREATING SLIDING WINDOWS (max_len=40)...\n----------------------------------------------------------------------\n‚úÖ Examples: 39,665\n   Shape: (39665, 40)\n‚úÖ ƒê√£ t·∫°o Dataset PyTorch v·ªõi 39,665 v√≠ d·ª• hu·∫•n luy·ªán.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"## 5.5.Train,val,test","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# ‚öôÔ∏è CHIA T·∫¨P TRAIN / VAL / TEST T·ª™ T·∫¨P TRAIN BAN ƒê·∫¶U\n# =============================================================================\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\nSEED = 42\nBATCH_SIZE = 64\nSEQUENCE_LENGTH = 40\n\n# -------------------------------------------------------------------------\n# B∆∞·ªõc 1: t√°ch Test tr∆∞·ªõc, r·ªìi t√°ch Val t·ª´ ph·∫ßn c√≤n l·∫°i\n# -------------------------------------------------------------------------\nX_temp, X_test, Y_temp, Y_test = train_test_split(\n    X, Y,\n    test_size=0.1,   # 10% cho test\n    random_state=SEED,\n    shuffle=True\n)\n\nX_train, X_val, Y_train, Y_val = train_test_split(\n    X_temp, Y_temp,\n    test_size=0.1,   # 10% c·ªßa ph·∫ßn c√≤n l·∫°i ‚Üí ~9% t·ªïng\n    random_state=SEED,\n    shuffle=True\n)\n\nprint(f\"üìä Dataset Split:\")\nprint(f\"   - Train samples: {len(X_train):,}\")\nprint(f\"   - Val samples:   {len(X_val):,}\")\nprint(f\"   - Test samples:  {len(X_test):,}\")\n\n# -------------------------------------------------------------------------\n# B∆∞·ªõc 2: T·∫°o Dataset v√† DataLoader\n# -------------------------------------------------------------------------\ntrain_dataset = TextDataset(X_train, Y_train)\nval_dataset = TextDataset(X_val, Y_val)\ntest_dataset = TextDataset(X_test, Y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n\nprint(f\"‚úÖ Train loader: {len(train_loader)} batches\")\nprint(f\"‚úÖ Val loader:   {len(val_loader)} batches\")\nprint(f\"‚úÖ Test loader:  {len(test_loader)} batches\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:04:58.496754Z","iopub.execute_input":"2025-10-10T03:04:58.497158Z","iopub.status.idle":"2025-10-10T03:04:58.590663Z","shell.execute_reply.started":"2025-10-10T03:04:58.497132Z","shell.execute_reply":"2025-10-10T03:04:58.588574Z"}},"outputs":[{"name":"stdout","text":"üìä Dataset Split:\n   - Train samples: 32,128\n   - Val samples:   3,570\n   - Test samples:  3,967\n‚úÖ Train loader: 502 batches\n‚úÖ Val loader:   56 batches\n‚úÖ Test loader:  62 batches\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"## 6.LSTM MODEL","metadata":{}},{"cell_type":"code","source":"class LSTMTextGenerator(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2,\n                 dropout=0.3, embedding_matrix=None, trainable_emb=True):\n        super().__init__()\n        \n        # Embedding\n        if embedding_matrix is not None:\n            weights = torch.tensor(embedding_matrix, dtype=torch.float32)\n            self.embedding = nn.Embedding.from_pretrained(\n                weights, freeze=not trainable_emb, padding_idx=0\n            )\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        \n        self.embed_dim = self.embedding.embedding_dim\n        \n        # Layers\n        self.input_ln = nn.LayerNorm(self.embed_dim)\n        self.input_dropout = nn.Dropout(dropout)\n        \n        self.lstm = nn.LSTM(\n            self.embed_dim, hidden_dim, num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        \n        self.hidden_ln = nn.LayerNorm(hidden_dim)\n        self.hidden_dropout = nn.Dropout(dropout)\n        \n        # Project to embedding space for weight tying\n        self.hidden_to_embed = nn.Linear(hidden_dim, self.embed_dim)\n        nn.init.xavier_uniform_(self.hidden_to_embed.weight)\n        \n        self.output_ln = nn.LayerNorm(self.embed_dim)\n        self.output_dropout = nn.Dropout(dropout)\n        self.output_bias = nn.Parameter(torch.zeros(vocab_size))\n    \n    def forward(self, x):\n        # Embedding\n        emb = self.embedding(x)  # (B, T, E)\n        emb = self.input_ln(emb)\n        emb = self.input_dropout(emb)\n        \n        # LSTM\n        lstm_out, _ = self.lstm(emb)  # (B, T, H)\n        lstm_out = self.hidden_ln(lstm_out)\n        lstm_out = self.hidden_dropout(lstm_out)\n        \n        # Project + residual\n        proj = self.hidden_to_embed(lstm_out)\n        proj = proj + emb\n        proj = self.output_ln(proj)\n        proj = self.output_dropout(proj)\n        \n        # Weight tying\n        logits = torch.matmul(proj, self.embedding.weight.t()) + self.output_bias\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:05:04.049644Z","iopub.execute_input":"2025-10-10T03:05:04.050778Z","iopub.status.idle":"2025-10-10T03:05:04.066728Z","shell.execute_reply.started":"2025-10-10T03:05:04.050737Z","shell.execute_reply":"2025-10-10T03:05:04.064598Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üñ•Ô∏è  S·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:05:07.534878Z","iopub.execute_input":"2025-10-10T03:05:07.535334Z","iopub.status.idle":"2025-10-10T03:05:07.545905Z","shell.execute_reply.started":"2025-10-10T03:05:07.535287Z","shell.execute_reply":"2025-10-10T03:05:07.542604Z"}},"outputs":[{"name":"stdout","text":"üñ•Ô∏è  S·ª≠ d·ª•ng thi·∫øt b·ªã: cpu\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"model = LSTMTextGenerator(\n    vocab_size=len(stoi),\n    embed_dim=300,\n    hidden_dim=256,\n    num_layers=2,\n    dropout=0.3,\n    embedding_matrix=embedding_matrix,\n    trainable_emb=True\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:05:11.814484Z","iopub.execute_input":"2025-10-10T03:05:11.815034Z","iopub.status.idle":"2025-10-10T03:05:11.917006Z","shell.execute_reply.started":"2025-10-10T03:05:11.814998Z","shell.execute_reply":"2025-10-10T03:05:11.915629Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"## 7.Training","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, path='best_model.pt'):\n        self.patience = patience\n        self.path = path\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n    \n    def __call__(self, val_loss, model):\n        if self.best_loss is None or val_loss < self.best_loss:\n            self.best_loss = val_loss\n            torch.save(model.state_dict(), self.path)\n            self.counter = 0\n            print(f\"   üíæ Checkpoint saved\")\n        else:\n            self.counter += 1\n            print(f\"   ‚è≥ EarlyStopping: {self.counter}/{self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n\ndef train_model(model, train_loader, val_loader, num_epochs=50,\n                lr=3e-4, clip_norm=5.0, patience=5, device='cuda'):\n    \"\"\"Training loop\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(\"üöÄ TRAINING START\")\n    print(f\"{'='*70}\\n\")\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, 'min', factor=0.5, patience=3, verbose=True\n    )\n    \n    early_stopping = EarlyStopping(patience=patience)\n    history = {'train_loss': [], 'train_ppl': [], 'val_loss': [], 'val_ppl': []}\n    \n    for epoch in range(1, num_epochs + 1):\n        # Train\n        model.train()\n        total_loss = 0.0\n        \n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            \n            optimizer.zero_grad()\n            logits = model(x)\n            \n            B, T, V = logits.size()\n            loss = criterion(logits.view(B*T, V), y.view(B*T))\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        train_loss = total_loss / len(train_loader)\n        train_ppl = math.exp(min(train_loss, 100))\n        \n        # Validation\n        model.eval()\n        total_val = 0.0\n        \n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                logits = model(x)\n                B, T, V = logits.size()\n                total_val += criterion(logits.view(B*T, V), y.view(B*T)).item()\n        \n        val_loss = total_val / len(val_loader)\n        val_ppl = math.exp(min(val_loss, 100))\n        \n        history['train_loss'].append(train_loss)\n        history['train_ppl'].append(train_ppl)\n        history['val_loss'].append(val_loss)\n        history['val_ppl'].append(val_ppl)\n        \n        print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n              f\"Train: {train_loss:.4f} (PPL {train_ppl:7.2f}) | \"\n              f\"Val: {val_loss:.4f} (PPL {val_ppl:7.2f})\")\n        \n        scheduler.step(val_loss)\n        early_stopping(val_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"\\n‚ö†Ô∏è  Early stopping!\")\n            break\n    \n    model.load_state_dict(torch.load('best_model.pt'))\n    return model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:05:18.296675Z","iopub.execute_input":"2025-10-10T03:05:18.297140Z","iopub.status.idle":"2025-10-10T03:05:18.315011Z","shell.execute_reply.started":"2025-10-10T03:05:18.297102Z","shell.execute_reply":"2025-10-10T03:05:18.313641Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# KH·ªûI T·∫†O TR·ªåNG S·ªê (T√πy ch·ªçn, nh∆∞ng n√™n l√†m)\ndef initialize_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.LSTM):\n        for name, param in m.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\nmodel.apply(initialize_weights)\n\n# Chuy·ªÉn model sang DEVICE (CPU trong tr∆∞·ªùng h·ª£p n√†y)\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:05:20.754808Z","iopub.execute_input":"2025-10-10T03:05:20.755303Z","iopub.status.idle":"2025-10-10T03:05:20.947027Z","shell.execute_reply.started":"2025-10-10T03:05:20.755270Z","shell.execute_reply":"2025-10-10T03:05:20.945631Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"LSTMTextGenerator(\n  (embedding): Embedding(2039, 300, padding_idx=0)\n  (input_ln): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n  (input_dropout): Dropout(p=0.3, inplace=False)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.3)\n  (hidden_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (hidden_dropout): Dropout(p=0.3, inplace=False)\n  (hidden_to_embed): Linear(in_features=256, out_features=300, bias=True)\n  (output_ln): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n  (output_dropout): Dropout(p=0.3, inplace=False)\n)"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"# ƒê·ªãnh nghƒ©a c√°c tham s·ªë Hu·∫•n luy·ªán\nNUM_EPOCHS = 15\nLEARNING_RATE = 1e-3\nCLIP_VALUE = 5.0 # Gi√° tr·ªã cho Gradient Clipping\nPATIENCE = 5     # S·ªë epoch ch·ªù tr∆∞·ªõc khi d·ª´ng s·ªõm (Early Stopping)\n\nprint(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH...\")\nprint(\"-\" * 50)\n\n# >> G·ªåI H√ÄM HU·∫§N LUY·ªÜN:\nbest_model, history = train_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    num_epochs=NUM_EPOCHS, \n    lr=LEARNING_RATE, \n    clip_norm=CLIP_VALUE, \n    patience=PATIENCE, \n    device=DEVICE # Bi·∫øn DEVICE ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü B∆∞·ªõc 1\n)\n\nprint(\"-\" * 50)\nprint(\"‚úÖ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T.\")\nprint(f\"M√¥ h√¨nh t·ªët nh·∫•t (Best Model) ƒë√£ ƒë∆∞·ª£c l∆∞u.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:05:29.158712Z","iopub.execute_input":"2025-10-10T03:05:29.159068Z","iopub.status.idle":"2025-10-10T03:48:57.484401Z","shell.execute_reply.started":"2025-10-10T03:05:29.159046Z","shell.execute_reply":"2025-10-10T03:48:57.480673Z"}},"outputs":[{"name":"stdout","text":"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH...\n--------------------------------------------------\n\n======================================================================\nüöÄ TRAINING START\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch   1/15 | Train: 5.3382 (PPL  208.13) | Val: 3.5907 (PPL   36.26)\n   üíæ Checkpoint saved\nEpoch   2/15 | Train: 3.4337 (PPL   30.99) | Val: 2.2240 (PPL    9.24)\n   üíæ Checkpoint saved\nEpoch   3/15 | Train: 2.5451 (PPL   12.74) | Val: 1.5571 (PPL    4.75)\n   üíæ Checkpoint saved\nEpoch   4/15 | Train: 2.0663 (PPL    7.90) | Val: 1.2498 (PPL    3.49)\n   üíæ Checkpoint saved\nEpoch   5/15 | Train: 1.7792 (PPL    5.93) | Val: 1.0781 (PPL    2.94)\n   üíæ Checkpoint saved\nEpoch   6/15 | Train: 1.6054 (PPL    4.98) | Val: 0.9751 (PPL    2.65)\n   üíæ Checkpoint saved\nEpoch   7/15 | Train: 1.4792 (PPL    4.39) | Val: 0.9107 (PPL    2.49)\n   üíæ Checkpoint saved\nEpoch   8/15 | Train: 1.3893 (PPL    4.01) | Val: 0.8616 (PPL    2.37)\n   üíæ Checkpoint saved\nEpoch   9/15 | Train: 1.3218 (PPL    3.75) | Val: 0.8343 (PPL    2.30)\n   üíæ Checkpoint saved\nEpoch  10/15 | Train: 1.2662 (PPL    3.55) | Val: 0.7978 (PPL    2.22)\n   üíæ Checkpoint saved\nEpoch  11/15 | Train: 1.2233 (PPL    3.40) | Val: 0.7757 (PPL    2.17)\n   üíæ Checkpoint saved\nEpoch  12/15 | Train: 1.1859 (PPL    3.27) | Val: 0.7567 (PPL    2.13)\n   üíæ Checkpoint saved\nEpoch  13/15 | Train: 1.1534 (PPL    3.17) | Val: 0.7449 (PPL    2.11)\n   üíæ Checkpoint saved\nEpoch  14/15 | Train: 1.1261 (PPL    3.08) | Val: 0.7272 (PPL    2.07)\n   üíæ Checkpoint saved\nEpoch  15/15 | Train: 1.1006 (PPL    3.01) | Val: 0.7141 (PPL    2.04)\n   üíæ Checkpoint saved\n--------------------------------------------------\n‚úÖ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T.\nM√¥ h√¨nh t·ªët nh·∫•t (Best Model) ƒë√£ ƒë∆∞·ª£c l∆∞u.\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"## 8.TEXT GENERATION","metadata":{}},{"cell_type":"code","source":"import torch\n\nimport torch\nimport torch.nn.functional as F\nimport random\n\nimport torch\nimport torch.nn.functional as F\nimport random\n\ndef generate_text(\n    model, seed, stoi, itos,\n    max_len=50, device=\"cpu\",\n    temperature=0.8, top_k=10,\n    repetition_penalty=1.2,\n    stop_tokens=[\"<eos>\"],\n    verbose=False\n):\n    \"\"\"Generate text with top-k sampling, repetition penalty, and early stopping.\"\"\"\n    model.eval()\n    \n    UNK_ID = stoi.get(\"<unk>\", 0)\n    seed = seed.lower().strip()\n    tokens = [stoi.get(w, UNK_ID) for w in seed.split()]\n    generated = tokens[:]\n\n    with torch.no_grad():\n        for step in range(max_len):\n            x = torch.tensor([generated], dtype=torch.long, device=device)\n            logits = model(x)[0, -1, :] / temperature\n\n            # Repetition penalty\n            for t in set(generated):\n                logits[t] /= repetition_penalty\n\n            probs = F.softmax(logits, dim=-1)\n            top_k_probs, top_k_ids = torch.topk(probs, k=top_k)\n            top_k_probs = top_k_probs / top_k_probs.sum()\n\n            next_id = random.choices(\n                top_k_ids.cpu().tolist(),\n                weights=top_k_probs.cpu().tolist()\n            )[0]\n\n            generated.append(next_id)\n\n            # Early stop if meet stop token\n            if itos[next_id] in stop_tokens:\n                break\n\n            if verbose:\n                print(f\"Step {step}: {itos[next_id]}\")\n\n    words = [itos[i] if i < len(itos) else \"<unk>\" for i in generated]\n    return ' '.join(words)\n\n\n\n\n# ==============================\n# üå∏ V√≠ d·ª• s·ª≠ d·ª•ng:\n# ==============================\nseeds = [\n    \"Em y√™u anh\",\n    \"Tr·ªùi ƒë√™m nay\",\n    \"Gi·ªçt m∆∞a r∆°i\",\n    \"Anh v·∫´n nh·ªõ\",\n    \"M·ªôt ng√†y n√†o ƒë√≥\"\n]\n\nfor s in seeds:\n    print(\"Seed:\", s)\n    try:\n        generated_text = generate_text(\n            model=best_model,\n            seed=s,\n            stoi=stoi,\n            itos=itos,\n            max_len=50,\n            device=device\n        )\n        print(\"Generated:\", generated_text)\n    except Exception as e:\n        print(\"‚ö†Ô∏è Please adapt generate_text() to your model:\", e)\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T03:58:14.656906Z","iopub.execute_input":"2025-10-10T03:58:14.658078Z","iopub.status.idle":"2025-10-10T03:58:15.957537Z","shell.execute_reply.started":"2025-10-10T03:58:14.658044Z","shell.execute_reply":"2025-10-10T03:58:15.956661Z"}},"outputs":[{"name":"stdout","text":"Seed: Em y√™u anh\nGenerated: em y√™u anh , yeah li·ªáu m√¨nh c√≤n y√™u nhau th√¨ kh√¥ng th·∫•m bi·∫øt ai c√≤n nh·ªõ ai , khu√¥n m·∫∑t ƒë√°ng th∆∞∆°ng c√°nh hoa √∫a t√†n b·ª©c tranh v√©n m√†n b√≥ng ai xa ng√∫t ng√†n n∆∞·ªõc m·∫Øt r∆°i ·ª©a tr√†n s·∫ßu l√†n mi kh√©p t√¨nh bu·ªìn ai √©p bi·∫øt ƒëi v·ªÅ ch·ªën\n------------------------------------------------------------\nSeed: Tr·ªùi ƒë√™m nay\nGenerated: tr·ªùi ƒë√™m nay sao em kh√¥ng vui l√™n ti·∫øng y√™u ·∫•m √™m nh√¨n l·∫°i ni·ªÅm tin t·ª´ng trao gi·ªù sao sau bao ngu mu·ªôi sai l·∫ßm anh v·∫´n y·∫øu m·ªÅm l√† v·∫øt th∆∞∆°ng l√≤ng x√≥t xa ƒë·∫øn n∆°i ch√∫ng ta , nh∆∞ng m√† v∆∞·ª£t qua h·∫øt nh·ªØng nghƒ© suy con ƒë∆∞·ªùng sau n√†y\n------------------------------------------------------------\nSeed: Gi·ªçt m∆∞a r∆°i\nGenerated: gi·ªçt m∆∞a r∆°i tr√™n <unk> tr∆∞·ªùng tu·ªïi th∆° nh∆∞ n·∫Øng ·∫•m √°p m√πa ƒë√¥ng \" \" t·ª± t√¨nh c√≥ ng∆∞·ªùi y√™u ch∆∞a v∆°i nh·ªõ nh·ªØng l√∫c h·ª©a h·∫πn , l√∫c h·ª©a h·∫πn m√† th√¥i ch·∫≥ng c√≤n g√¨ ƒë√¢u , right baby , with you ( baby , with you ) i said \"\n------------------------------------------------------------\nSeed: Anh v·∫´n nh·ªõ\nGenerated: anh v·∫´n nh·ªõ em t·ª´ng thu·ªôc v·ªÅ anh n∆∞·ªõc m·∫Øt anh kh√¥ng th·ªÉ thi·∫øu v·∫Øng ƒë∆∞·ª£c em b·ªüi v√¨ anh r·∫•t mu·ªën ta li·ªáu c√≥ th·ªÉ quay v·ªÅ nh∆∞ ng√†y x∆∞a , ch·∫Øc c√≥ l·∫Ω m·ªói t√¥i y√™u anh anh l√† ai m√† t√¥i c·ª© lu√¥n lu√¥n ƒë·ª£i t·ª´ khi n√†o bi·∫øt y√™u\n------------------------------------------------------------\nSeed: M·ªôt ng√†y n√†o ƒë√≥\nGenerated: m·ªôt ng√†y n√†o ƒë√≥ , ch·∫≥ng hi·ªÉu t√¨nh y√™u v√¥ th·ª©c ƒë√£ bao nhi√™u l·∫ßn n·∫øm cay ƒë·∫Øng khi y√™u c·∫£m gi√°c l√∫c ·∫•y s·∫Ω ra sao , sao anh ch∆∞a v·ªÅ nay t·ª´ng h·∫πn , em ch·ªù th√¥i ƒë∆∞·ªùng ƒë√¢u n√¥ng n·ªói ƒë·∫øn h√¥m nay it ' s your birthday ( hey ,\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"## 9.VISUALIZATION & REPORT","metadata":{}},{"cell_type":"code","source":"for x, y in test_loader:\n    print(\"x shape:\", x.shape)\n    print(\"x sample:\", x[0][:10])  # 10 token ƒë·∫ßu c·ªßa m·∫´u ƒë·∫ßu ti√™n\n    print(\"y shape:\", y.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T04:06:46.465898Z","iopub.execute_input":"2025-10-10T04:06:46.467143Z","iopub.status.idle":"2025-10-10T04:06:46.495714Z","shell.execute_reply.started":"2025-10-10T04:06:46.467107Z","shell.execute_reply":"2025-10-10T04:06:46.494171Z"}},"outputs":[{"name":"stdout","text":"x shape: torch.Size([64, 40])\nx sample: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\ny shape: torch.Size([64, 40])\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"\"\"\"\nH√†m ƒë√°nh gi√° model tr√™n test set v·ªõi metric Perplexity\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport math\nimport time\nfrom tqdm import tqdm\n\n\ndef evaluate_on_test(model, test_loader, device='cuda', verbose=True):\n    \"\"\"\n    ƒê√°nh gi√° model tr√™n test set\n    \n    Args:\n        model: LSTM model ƒë√£ train\n        test_loader: DataLoader cho test set\n        device: 'cuda' or 'cpu'\n        verbose: In chi ti·∫øt hay kh√¥ng\n    \n    Returns:\n        dict: {\n            'test_loss': float,\n            'test_perplexity': float,\n            'num_batches': int,\n            'num_samples': int\n        }\n    \"\"\"\n    \n    if verbose:\n        print(f\"\\n{'='*70}\")\n        print(\"üìä EVALUATING ON TEST SET\")\n        print(f\"{'='*70}\\n\")\n    \n    model.eval()\n    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')  # sum ƒë·ªÉ t√≠nh ch√≠nh x√°c\n    \n    total_loss = 0.0\n    total_tokens = 0\n    num_batches = 0\n    \n    start_time = time.time()\n    \n    with torch.no_grad():\n        # Wrap v·ªõi tqdm n·∫øu verbose\n        iterator = tqdm(test_loader, desc=\"Testing\") if verbose else test_loader\n        \n        for batch_idx, (x, y) in enumerate(iterator):\n            x, y = x.to(device), y.to(device)\n            \n            # Forward pass\n            logits = model(x)  # (B, T, V)\n            B, T, V = logits.size()\n            \n            # Flatten\n            logits_flat = logits.view(B * T, V)\n            targets_flat = y.view(B * T)\n            \n            # Calculate loss (sum, not mean)\n            loss = criterion(logits_flat, targets_flat)\n            \n            # Count non-padding tokens\n            non_pad_tokens = (targets_flat != 0).sum().item()\n            \n            total_loss += loss.item()\n            total_tokens += non_pad_tokens\n            num_batches += 1\n    \n    elapsed_time = time.time() - start_time\n    \n    # Calculate metrics\n    avg_loss = total_loss / total_tokens  # Loss per token\n    perplexity = math.exp(min(avg_loss, 100))  # Cap ƒë·ªÉ tr√°nh overflow\n    \n    # Calculate samples (approximate - m·ªói sample c√≥ th·ªÉ c√≥ s·ªë tokens kh√°c nhau)\n    num_samples = num_batches * test_loader.batch_size\n    \n    # Print results\n    if verbose:\n        print(f\"\\n{'='*70}\")\n        print(\"‚úÖ TEST RESULTS\")\n        print(f\"{'='*70}\")\n        print(f\"Test Loss:        {avg_loss:.6f}\")\n        print(f\"Test Perplexity:  {perplexity:.4f}\")\n        print(f\"Num Batches:      {num_batches:,}\")\n        print(f\"Num Samples:      {num_samples:,}\")\n        print(f\"Total Tokens:     {total_tokens:,}\")\n        print(f\"Time Elapsed:     {elapsed_time:.2f}s\")\n        print(f\"{'='*70}\\n\")\n    \n    results = {\n        'test_loss': avg_loss,\n        'test_perplexity': perplexity,\n        'num_batches': num_batches,\n        'num_samples': num_samples,\n        'total_tokens': total_tokens,\n        'time_elapsed': elapsed_time\n    }\n    \n    return results\nresults = evaluate_on_test(model, test_loader, device=DEVICE)\nprint(f\"Final Test Perplexity: {results['test_perplexity']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T04:29:43.990350Z","iopub.execute_input":"2025-10-10T04:29:43.992275Z","iopub.status.idle":"2025-10-10T04:29:50.856347Z","shell.execute_reply.started":"2025-10-10T04:29:43.992188Z","shell.execute_reply":"2025-10-10T04:29:50.855135Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüìä EVALUATING ON TEST SET\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62/62 [00:06<00:00,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n‚úÖ TEST RESULTS\n======================================================================\nTest Loss:        0.680008\nTest Perplexity:  1.9739\nNum Batches:      62\nNum Samples:      3,968\nTotal Tokens:     52,442\nTime Elapsed:     6.84s\n======================================================================\n\nFinal Test Perplexity: 1.97\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":95}]}